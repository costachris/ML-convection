{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the neural network / random forest and run it on testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test that I am extracting the parameters correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_test = r_mlp.predict(x3)\n",
    "out_test = scaler_y.inverse_transform(out_test)\n",
    "w1 = r_mlp.get_parameters()[0].weights\n",
    "w2 = r_mlp.get_parameters()[1].weights\n",
    "w3 = r_mlp.get_parameters()[2].weights\n",
    "b1 = r_mlp.get_parameters()[0].biases\n",
    "b2 = r_mlp.get_parameters()[1].biases\n",
    "b3 = r_mlp.get_parameters()[2].biases\n",
    "\n",
    "xscale_min = scaler_x.data_min_\n",
    "xscale_max = scaler_x.data_max_\n",
    "yscale_absmax = scaler_y.max_abs_\n",
    "\n",
    "out_test_check = np.dot(x3,w1) + b1\n",
    "out_test_check[out_test_check<0] = 0\n",
    "out_test_check = np.dot(out_test_check,w2) + b2\n",
    "out_test_check[out_test_check<0] = 0\n",
    "out_test_check = np.dot(out_test_check,w3) + b3\n",
    "\n",
    "out_test_check = out_test_check*yscale_absmax\n",
    "ptl.plot(out_test-out_test_check)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that I understand what classification is doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "c_w1=c_mlp.get_parameters()[0].weights\n",
    "c_w2=c_mlp.get_parameters()[1].weights\n",
    "c_w3=c_mlp.get_parameters()[2].weights\n",
    "c_b1=c_mlp.get_parameters()[0].biases\n",
    "c_b2=c_mlp.get_parameters()[1].biases\n",
    "c_b3=c_mlp.get_parameters()[2].biases\n",
    "\n",
    "out_test_check = np.dot(x1            ,c_w1) + c_b1\n",
    "out_test_check[out_test_check<0] = 0\n",
    "out_test_check = np.dot(out_test_check,c_w2) + c_b2\n",
    "out_test_check[out_test_check<0] = 0\n",
    "out_test_check = np.dot(out_test_check,c_w3) + c_b3\n",
    "\n",
    "expo =  np.exp(out_test_check)\n",
    "expos = np.sum(expo,axis=1)\n",
    "\n",
    "#foo=np.empty((x1.shape[0], 2))\n",
    "foo[:,0] = expo[:,0]/expos\n",
    "foo[:,1] = expo[:,1]/expos\n",
    "ff=np.zeros(x1.shape[0])\n",
    "ff[foo[:,1]>0.5]=1.\n",
    "print(x1.shape)\n",
    "ee=c_mlp.predict(x1)\n",
    "ee=np.squeeze(ee)\n",
    "print(ee.shape)\n",
    "print(ff.shape)\n",
    "print(np.sum(np.logical_and(ff==0,ee==1)))\n",
    "print(np.sum(np.logical_and(ff==1,ee==0)))\n",
    "\n",
    "rexpo = np.exp(out_test_check[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,40))\n",
    "_,ax = plt.subplots(lev.size,2,sharex=True)\n",
    "for i in range(lev.size):\n",
    "    step=.05\n",
    "    bins=np.arange(-1,1+step,step)\n",
    "    n,bins,_ =ax[i,0].hist(unpack(x_train_norm,'T')[:,i],bins=bins,facecolor='yellow',alpha=0.5,normed=True)\n",
    "    n2,bins2,_=ax[i,1].hist(unpack(y_train_norm,'T')[:,i],bins=bins,facecolor='blue'  ,alpha=0.5,normed=True)\n",
    "\n",
    "\n",
    "    ax[i,0].set_xlim((-1,1))\n",
    "    ax[i,0].set_ylim(0,np.amax(n))\n",
    "    ax[i,1].set_ylim(0,np.amax(n2))\n",
    "\n",
    "\n",
    "    #ax[i,0].set_ylim([-1,1])\n",
    "    print(np.amax(n))\n",
    "    print(np.amax(n2))\n",
    "    #ax[i,1].hist(unpack(x_train_norm,'q')[:,i]*step,bins=np.arange(-1,1+step,step),facecolor='yellow',alpha=0.5,normed=True)\n",
    "    #ax[i,1].hist(unpack(y_train_norm,'q')[:,i]*step,bins=np.arange(-1,1+step,step),facecolor='blue'  ,alpha=0.5,normed=True)\n",
    "    #ax[i,1].set_xlim([-1,1])\n",
    "\n",
    "\n",
    "\n",
    "    #plt.subplot(lev.size,2,i+1+lev.size)\n",
    "    #plt.hist(y_train_norm[:,i],100,facecolor='green')\n",
    "    #ax[i,0].get_yaxis().set_visible(False)\n",
    "\n",
    "    #n, bins, patches = plt.hist(y_train_norm[:,28], 100, normed=1, facecolor='green', alpha=0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the neural network on the training data\n",
    "hidden_neurons = 100\n",
    "num_loops = 2#int(.1*np.power(10,6))\n",
    "print(num_loops)\n",
    "out_train_norm, model = nn2.run(x_train_norm, y_train_norm, hidden_neurons, print_loss=True, num_passes=num_loops,learning_rate=.01,regularization=.01)\n",
    "\n",
    "# Run the (trained) neural network on the test data\n",
    "out_test_norm, _ = nn2.fwd_prop(x_test_norm, model.w2, model.b2, model.w3, model.b3)\n",
    "\n",
    "#Unnormalize model output\n",
    "out_train = nn.unnorm(out_train_norm, ytrainmin, ytrainmax)\n",
    "out_test  = nn.unnorm(out_test_norm,  ytestmin,  ytestmax )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate out of bag error importance for random forest regressor\n",
    "\n",
    "\n",
    "oob_mlp = RandomForestRegressor(n_estimators=30)\n",
    "\n",
    "oob_mlp.fit(x2,y2)\n",
    "indmin=np.argmin(oob_mlp.feature_importances_)\n",
    "print(indmin)\n",
    "print(np.min(oob_mlp.feature_importances_))\n",
    "print(oob_mlp.score(x3,y3))\n",
    "x2 = np.delete(x2,indmin,1)\n",
    "\n",
    "plt.plot(unpack(oob_mlp.feature_importances_,'T'),lev,label='T')\n",
    "plt.plot(unpack(oob_mlp.feature_importances_,'q'),lev,label='q')\n",
    "plt.ylim((1,0))\n",
    "plt.show()\n",
    "oob_mlp2 = RandomForestRegressor(n_estimators=30)\n",
    "oob_mlp2.fit(x2[:,oob_mlp.feature_importances_>0.],cv2)\n",
    "\n",
    "#oob_mlp2.feature_importances_.shape\n",
    "np.argmin(oob_mlp.feature_importances_)#[oob_mlp.feature_importances_>0.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "do_nn_classifier = False\n",
    "do_rf_classifier = False\n",
    "do_randomforest  = False\n",
    "do_neuralnetwork = True\n",
    "do_my_nn         = False\n",
    "\n",
    "errors = []\n",
    "def store_stats( avg_train_error, **_):\n",
    "    if np.mod(len(errors),100)==0:\n",
    "        print(\"Epoch #%5d, Error=%.4f\" %(len(errors),avg_train_error))\n",
    "    errors.append(( avg_train_error))\n",
    "    \n",
    "\n",
    "# Preclassifier to determine if convection is occurring or not\n",
    "if do_nn_classifier:\n",
    "    classifier = []\n",
    "    classifier = sknn.mlp.Classifier(layers=[sknn.mlp.Layer(\"Tanh\",units=100),sknn.mlp.Layer(\"Tanh\",units=100),sknn.mlp.Layer(\"Tanh\",units=100),\n",
    "                                             sknn.mlp.Layer(\"Softmax\")],learning_rate=0.01,n_iter=10000,batch_size=100,learning_rule='momentum',n_stable=8000,\n",
    "                                     callback={'on_epoch_finish': store_stats})\n",
    "    classifier.fit(x1,y1cv)\n",
    "    class_train=classifier.predict(x1)\n",
    "    class_test=classifier.predict(x2)\n",
    "    plt.plot(errors)\n",
    "    plt.title('Error')\n",
    "    plt.xlabel('Iteration #')\n",
    "    plt.show()\n",
    "\n",
    "if do_rf_classifier: #randomforest_classifier\n",
    "    classifier = RandomForestClassifier(n_estimators=15)\n",
    "    classifier.fit(x1,y1cv.ravel())\n",
    "    class_train = classifier.predict(x1)\n",
    "    class_test = classifier.predict(x2)\n",
    "    # Plot feature importance\n",
    "    plt.plot(classifier.feature_importances_[0:30],lev,label=\"T\")\n",
    "    plt.plot(classifier.feature_importances_[30:60],lev,label=\"q\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Random Forest\n",
    "if do_randomforest:\n",
    "    model = RandomForestRegressor(n_estimators=75)\n",
    "    model.fit(x1,y1)\n",
    "    out_train = model.predict(x1)\n",
    "    out_test = model.predict(x2)\n",
    "    # Plot feature importance\n",
    "    plt.plot(model.feature_importances_[0:30],lev,label=\"T\")\n",
    "    plt.plot(model.feature_importances_[30:60],lev,label=\"q\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "# Neural Network through the lasagne package (via scikit-learn neural network)\n",
    "if do_neuralnetwork:\n",
    "    neur=[]\n",
    "    neur=sknn.mlp.Regressor(layers=[sknn.mlp.Layer(\"Tanh\",units=200),\n",
    "                                    sknn.mlp.Layer(\"Tanh\",units=200),sknn.mlp.Layer(\"Linear\")],\n",
    "                            n_iter=100,batch_size=100,callback={'on_epoch_finish': store_stats},\n",
    "                            learning_rate=.01,learning_rule=\"momentum\",learning_momentum=.7)\n",
    "    neur.fit(x1,y1)\n",
    "    out_train=neur.predict(x1)\n",
    "    out_test=neur.predict(x2)\n",
    "    plt.plot(errors)\n",
    "    plt.title('Mean Squared Error')\n",
    "    plt.xlabel('Iteration #')\n",
    "    plt.show()\n",
    "\n",
    "# Homemade neural network\n",
    "if do_my_nn:\n",
    "    hidden_neurons = 100\n",
    "    num_loops = int(np.power(10,5))\n",
    "    out_train, model = nn2.run(x1, y1, hidden_neurons, print_loss=True, num_passes=num_loops,learning_rate=.01,regularization=.01)\n",
    "    # Run the (trained) neural network on the test data\n",
    "    out_test, _ = nn2.fwd_prop(x2, model.w2, model.b2, model.w3, model.b3)\n",
    "\n",
    "#Unnormalize model output\n",
    "#out_train = y_scaler.inverse_transform(out_train)\n",
    "#out_test =  y_scaler.inverse_transform(out_test )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
